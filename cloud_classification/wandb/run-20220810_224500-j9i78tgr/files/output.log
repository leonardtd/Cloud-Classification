Traceback (most recent call last):
  File "/home/ltorres/leo/Cloud-Classification/cloud_classification/train.py", line 96, in <module>
    train(architecture,
  File "/home/ltorres/leo/Cloud-Classification/cloud_classification/train.py", line 64, in train
    trainer = ModelTrainer(architecture, config, use_wandb, data_loaders)
  File "/home/ltorres/leo/Cloud-Classification/cloud_classification/src/model.py", line 42, in __init__
    self.model = build_model(self.architecture, self.config)
  File "/home/ltorres/leo/Cloud-Classification/cloud_classification/src/model.py", line 31, in build_model
    return model.to(config["hardware"]["device"])
  File "/home/ltorres/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/home/ltorres/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/home/ltorres/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/home/ltorres/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 1; 10.76 GiB total capacity; 9.24 MiB already allocated; 73.62 MiB free; 22.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF